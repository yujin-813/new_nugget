import logging
import pandas as pd
import os
import openai
import json
import re
import numpy as np
import warnings
from db_manager import DBManager

class FileAnalysisEngine:
    def __init__(self):
        pass

    def process(self, question, file_path, conversation_id=None, prev_source=None, beginner_mode=False):
        logging.info(f"ğŸ“ [File Engine] Analyzing: {file_path}")
        if not file_path or not os.path.exists(file_path):
             return {"message": "íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status": "error"}
             
        try:
            # 1. Load Data
            df = pd.read_csv(file_path) if file_path.endswith('.csv') else pd.read_excel(file_path)
            file_name = os.path.basename(file_path)
            dataset_period = self._infer_dataset_period(df)
            
            # 2. Context / State Management
            state = {}
            if conversation_id:
                # [Phase 5] Restore last intent if follow-up
                state = DBManager.load_last_state(conversation_id, "file") or {}
                
            # 3. Analyze Query (Level 1-3 Strategy)
            result_df, message, intent, analysis_meta = self._analyze_query(df, question, state)
            if beginner_mode:
                message = self._make_beginner_message(message, intent, analysis_meta)
            
            # 4. Transform for Visualization
            plot_data = self._transform_to_plot_data(result_df, intent)
            followups = self._build_followups(df, intent, analysis_meta)
            raw_limit = self._raw_limit_by_intent(intent, analysis_meta)
            intent_plan = {
                "source": "file",
                "intent_type": intent.get("type"),
                "group_col": analysis_meta.get("group_col"),
                "metric_col": analysis_meta.get("metric_col"),
                "op": analysis_meta.get("op"),
                "date_col": analysis_meta.get("date_col"),
                "period": analysis_meta.get("period"),
                "row_count": int(len(df)),
                "column_count": int(len(df.columns))
            }
            
            # 5. Save State
            if result_df is not None and conversation_id:
                # [Phase 5] Save Intent for Follow-up
                state["last_intent"] = intent
                state["last_analysis_meta"] = analysis_meta or {}
                DBManager.save_success_state(conversation_id, "file", state)

            period_text = analysis_meta.get("period") or dataset_period
            return {
                "message": message,
                "status": "ok",
                "plot_data": plot_data,
                "raw_data": result_df.head(raw_limit).where(pd.notnull(result_df), None).to_dict(orient='records') if result_df is not None else [],
                "followup_suggestions": followups,
                "period": period_text,
                "file_name": file_name,
                "data_label": f"FILE Â· {file_name}",
                "collection_period": dataset_period,
                "intent_plan": intent_plan
            }
            
        except Exception as e:
            logging.error(f"[File Engine Error] {e}")
            return {"message": f"íŒŒì¼ ë¶„ì„ ì˜¤ë¥˜: {e}", "status": "error"}

    def _analyze_query(self, df, question, state):
        # [Phase 5] 3-Level Processing Strategy
        intent = self._detect_intent(question, state)
        col_count = self._detect_column_count(df, question)
        if col_count:
            intent["type"] = "column_count"
            intent["target_column"] = col_count
        q = str(question or "").lower()
        page_req = self._detect_page_request(question)
        if not page_req and isinstance(state, dict):
            last_meta = state.get("last_analysis_meta", {}) or {}
            if last_meta.get("show_unique") and last_meta.get("target_column"):
                if any(k in q for k in ["ë‹¤ìŒ", "ê³„ì†", "ì´ì–´", "more", "next"]):
                    page_req = {"direction": "next", "size": int(last_meta.get("page_limit", 500) or 500)}
                elif any(k in q for k in ["ì´ì „", "ì•", "prev", "previous"]):
                    page_req = {"direction": "prev", "size": int(last_meta.get("page_limit", 500) or 500)}
        if page_req and isinstance(state, dict):
            last_meta = state.get("last_analysis_meta", {}) or {}
            if last_meta.get("show_unique") and last_meta.get("target_column"):
                last_offset = int(last_meta.get("page_offset", 0))
                limit = int(last_meta.get("page_limit", 500))
                step = int(page_req.get("size") or limit)
                if page_req.get("direction") == "next":
                    new_offset = max(0, last_offset + step)
                else:
                    new_offset = max(0, last_offset - step)
                intent["type"] = "column_probe"
                intent["target_column"] = last_meta.get("target_column")
                intent["preview_count"] = step
                intent["show_unique"] = True
                intent["offset"] = new_offset
        elif isinstance(state, dict):
            last_meta = state.get("last_analysis_meta", {}) or {}
            if last_meta.get("show_unique") and last_meta.get("target_column"):
                if any(k in q for k in ["ì „ì²´", "ëª©ë¡", "ëª¨ë‘", "ì „ì²´ ë³´ì—¬", "ë‹¤ ë³´ì—¬"]):
                    intent["type"] = "column_probe"
                    intent["target_column"] = last_meta.get("target_column")
                    intent["preview_count"] = 500
                    intent["show_unique"] = True
                    intent["offset"] = 0

        if self._is_preview_more_request(question):
            last_intent = (state.get("last_intent") or {}).get("type") if isinstance(state, dict) else None
            last_meta = state.get("last_analysis_meta", {}) if isinstance(state, dict) else {}
            if last_intent in {"column_probe", "schema", "columns_summary", "overview", "preview"}:
                intent["type"] = "preview_more"
                intent["target_column"] = last_meta.get("target_column")
                intent["preview_count"] = 10

        col_probe = self._detect_column_probe(df, question)
        if col_probe:
            intent["type"] = "column_probe"
            intent["target_column"] = col_probe.get("target_column")
            intent["preview_count"] = int(col_probe.get("preview_count", 5))
            intent["show_unique"] = bool(col_probe.get("show_unique", False))
        analysis_meta = {}
        
        # 1. Level 1 & 2: Skip LLM (Exploration & Aggregation)
        if intent["type"] in ["schema", "preview", "overview", "groupby", "aggregate", "distribution", "filter", "count_users", "count_admin", "columns_summary", "explain", "trend", "compare", "guidance", "column_probe", "column_count", "preview_more"]:
            result_df, analysis_meta = self._execute_aggregation(df, intent, state)
            
            if intent["type"] == "schema":
                row_count = int(analysis_meta.get("row_count", len(df)))
                col_count = int(analysis_meta.get("col_count", len(df.columns)))
                numeric_count = int(analysis_meta.get("numeric_count", 0))
                cat_count = int(analysis_meta.get("categorical_count", 0))
                date_count = int(analysis_meta.get("date_count", 0))
                bool_count = int(analysis_meta.get("boolean_count", 0))
                id_count = int(analysis_meta.get("identifier_count", 0))
                sample_cols = analysis_meta.get("sample_columns", [])
                numeric_cols = analysis_meta.get("numeric_columns", [])
                categorical_cols = analysis_meta.get("categorical_columns", [])
                date_cols = analysis_meta.get("date_columns", [])
                bool_cols = analysis_meta.get("boolean_columns", [])
                id_cols = analysis_meta.get("identifier_columns", [])
                sample_cols_txt = ", ".join(sample_cols[:6]) if sample_cols else ""
                numeric_cols_txt = ", ".join([str(c) for c in numeric_cols[:3]]) if numeric_cols else "-"
                cat_cols_txt = ", ".join([str(c) for c in categorical_cols[:3]]) if categorical_cols else "-"
                date_cols_txt = ", ".join([str(c) for c in date_cols[:3]]) if date_cols else "-"
                bool_cols_txt = ", ".join([str(c) for c in bool_cols[:3]]) if bool_cols else "-"
                id_cols_txt = ", ".join([str(c) for c in id_cols[:3]]) if id_cols else "-"
                msg = (
                    f"íŒŒì¼ êµ¬ì¡°ë¥¼ ê°„ë‹¨íˆ ì •ë¦¬í•˜ë©´ **{row_count:,}í–‰ / {col_count}ì»¬ëŸ¼**ì…ë‹ˆë‹¤. "
                    f"ìˆ˜ì¹˜í˜• {numeric_count}ê°œ(ê³„ì‚°ìš©), ë²”ì£¼í˜• {cat_count}ê°œ(ë¶„ë¥˜ìš©), ë‚ ì§œí˜• {date_count}ê°œ, ë¶ˆë¦¬ì–¸ {bool_count}ê°œ, ì‹ë³„ì {id_count}ê°œì…ë‹ˆë‹¤.\n"
                    f"- ìˆ˜ì¹˜í˜• ì˜ˆ: `{numeric_cols_txt}`\n"
                    f"- ë²”ì£¼í˜• ì˜ˆ: `{cat_cols_txt}`\n"
                    f"- ë‚ ì§œí˜• ì˜ˆ: `{date_cols_txt}`\n"
                    f"- ë¶ˆë¦¬ì–¸ ì˜ˆ: `{bool_cols_txt}`\n"
                    f"- ì‹ë³„ì ì˜ˆ: `{id_cols_txt}`\n"
                    f"- ëŒ€í‘œ ì»¬ëŸ¼: `{sample_cols_txt}`"
                )
                msg += self._build_preview_tail(analysis_meta)
            elif intent["type"] == "preview":
                msg = f"íŒŒì¼ì˜ ìƒìœ„ {len(result_df)}í–‰ ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤."
            elif intent["type"] == "overview":
                msg = f"íŒŒì¼ ì „ì²´ ê°œìš”: ì´ {len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼"
                msg += self._build_preview_tail(analysis_meta)
            elif intent["type"] == "guidance":
                msg = analysis_meta.get("guide_text") or "íŒŒì¼ ë¶„ì„ì„ ì‹œì‘í•  ìˆ˜ ìˆëŠ” ì¶”ì²œ ì§ˆë¬¸ì„ ë“œë¦´ê²Œìš”."
            elif intent["type"] == "count_users":
                user_count = int(analysis_meta.get("user_count", len(df)))
                id_col = analysis_meta.get("id_column")
                if id_col:
                    msg = f"ì´ íŒŒì¼ ê¸°ì¤€ ì‚¬ìš©ì ìˆ˜ëŠ” **{user_count}ëª…**ì…ë‹ˆë‹¤. (`{id_col}` ê¸°ì¤€)"
                else:
                    msg = f"ì´ íŒŒì¼ ê¸°ì¤€ ì‚¬ìš©ì ìˆ˜ëŠ” **{user_count}ëª…**ì…ë‹ˆë‹¤."
            elif intent["type"] == "count_admin":
                admin_count = int(analysis_meta.get("admin_count", 0))
                total_count = int(analysis_meta.get("total_count", len(df)))
                admin_cols = analysis_meta.get("admin_columns", [])
                ratio = (admin_count / total_count * 100) if total_count else 0.0
                col_text = f" ({', '.join(admin_cols)})" if admin_cols else ""
                msg = f"ê´€ë¦¬ì(ì–´ë“œë¯¼) ìˆ˜ëŠ” **{admin_count}ëª…**ì…ë‹ˆë‹¤{col_text}. ì „ì²´ ëŒ€ë¹„ **{ratio:.1f}%**ì…ë‹ˆë‹¤."
            elif intent["type"] == "columns_summary":
                numeric_count = int(analysis_meta.get("numeric_count", 0))
                categorical_count = int(analysis_meta.get("categorical_count", 0))
                date_count = int(analysis_meta.get("date_count", 0))
                bool_count = int(analysis_meta.get("boolean_count", 0))
                id_count = int(analysis_meta.get("identifier_count", 0))
                numeric_cols = analysis_meta.get("numeric_columns", [])
                categorical_cols = analysis_meta.get("categorical_columns", [])
                date_cols = analysis_meta.get("date_columns", [])
                bool_cols = analysis_meta.get("boolean_columns", [])
                id_cols = analysis_meta.get("identifier_columns", [])
                numeric_cols_txt = ", ".join([str(c) for c in numeric_cols[:3]]) if numeric_cols else "-"
                cat_cols_txt = ", ".join([str(c) for c in categorical_cols[:3]]) if categorical_cols else "-"
                date_cols_txt = ", ".join([str(c) for c in date_cols[:3]]) if date_cols else "-"
                bool_cols_txt = ", ".join([str(c) for c in bool_cols[:3]]) if bool_cols else "-"
                id_cols_txt = ", ".join([str(c) for c in id_cols[:3]]) if id_cols else "-"
                msg = (
                    f"ì´ íŒŒì¼ì—ëŠ” ì´ **{len(df.columns)}ê°œ ì»¬ëŸ¼**ì´ ìˆìŠµë‹ˆë‹¤. "
                    f"ìˆ˜ì¹˜í˜• **{numeric_count}ê°œ**(í•©ê³„/í‰ê·  ê³„ì‚°ìš©), "
                    f"ë²”ì£¼í˜• **{categorical_count}ê°œ**(~ë³„ ë¹„êµìš©), "
                    f"ë‚ ì§œí˜• **{date_count}ê°œ**, ë¶ˆë¦¬ì–¸ **{bool_count}ê°œ**, ì‹ë³„ì **{id_count}ê°œ**ì…ë‹ˆë‹¤.\n"
                    f"- ìˆ˜ì¹˜í˜• ì˜ˆ: `{numeric_cols_txt}`\n"
                    f"- ë²”ì£¼í˜• ì˜ˆ: `{cat_cols_txt}`\n"
                    f"- ë‚ ì§œí˜• ì˜ˆ: `{date_cols_txt}`\n"
                    f"- ë¶ˆë¦¬ì–¸ ì˜ˆ: `{bool_cols_txt}`\n"
                    f"- ì‹ë³„ì ì˜ˆ: `{id_cols_txt}`"
                )
                msg += self._build_preview_tail(analysis_meta)
            elif intent["type"] == "column_probe":
                tc = intent.get("target_column")
                pc = int(intent.get("preview_count", 5))
                if analysis_meta.get("show_unique"):
                    total_unique = int(analysis_meta.get("total_unique", len(result_df)))
                    shown = int(analysis_meta.get("shown_unique", len(result_df)))
                    start_idx = int(analysis_meta.get("page_offset", 0)) + 1
                    end_idx = min(int(analysis_meta.get("page_offset", 0)) + shown, total_unique)
                    msg = f"`{tc}` ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ëª©ë¡ì…ë‹ˆë‹¤. (ì´ {total_unique}ê°œ, í˜„ì¬ {start_idx}~{end_idx})"
                else:
                    msg = f"`{tc}` ì»¬ëŸ¼ì˜ ìƒìœ„ {pc}í–‰ ê°’ ë¯¸ë¦¬ë³´ê¸°ì…ë‹ˆë‹¤."
            elif intent["type"] == "column_count":
                tc = intent.get("target_column")
                c = int(analysis_meta.get("unique_count", 0))
                msg = f"`{tc}` ê¸°ì¤€ ê³ ìœ  ê°œìˆ˜ëŠ” **{c:,}ê°œ**ì…ë‹ˆë‹¤."
            elif intent["type"] == "preview_more":
                tc = intent.get("target_column")
                if tc and tc in df.columns:
                    msg = f"`{tc}` ì»¬ëŸ¼ì˜ ì¶”ê°€ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ 10í–‰)ì…ë‹ˆë‹¤."
                else:
                    msg = f"íŒŒì¼ì˜ ì¶”ê°€ ë¯¸ë¦¬ë³´ê¸°(ìƒìœ„ {len(result_df)}í–‰)ì…ë‹ˆë‹¤."
            elif intent["type"] == "explain":
                msg = analysis_meta.get("explain_text") or "ì§ì „ ë¶„ì„ ê²°ê³¼ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì˜ë¯¸ë¥¼ ì„¤ëª…ë“œë ¸ìŠµë‹ˆë‹¤."
            elif intent["type"] == "trend":
                date_col = analysis_meta.get("date_col")
                metric_col = analysis_meta.get("metric_col")
                msg = f"ì¼ì ì¶”ì´ ê²°ê³¼ì…ë‹ˆë‹¤. ({date_col} ê¸°ì¤€, ì§€í‘œ: {metric_col})"
            elif intent["type"] == "compare":
                group_col = analysis_meta.get("group_col")
                metric_col = analysis_meta.get("metric_col")
                msg = f"ë¹„êµ ê²°ê³¼ì…ë‹ˆë‹¤. ({group_col} ê¸°ì¤€, ì§€í‘œ: {metric_col})"
            elif intent["type"] in ["groupby", "distribution"]:
                group_col = analysis_meta.get("group_col")
                metric_col = analysis_meta.get("metric_col")
                op = analysis_meta.get("op")
                if group_col:
                    msg = f"ìš”ì²­í•˜ì‹  `{group_col}` ê¸°ì¤€ ì§‘ê³„ ê²°ê³¼ì…ë‹ˆë‹¤. ({op}: {metric_col})"
                else:
                    msg = f"ìš”ì²­í•˜ì‹  '{intent.get('keywords', ['ê·¸ë£¹'])[0]}' ê¸°ì¤€ ì§‘ê³„ ê²°ê³¼ì…ë‹ˆë‹¤."
            elif intent["type"] == "aggregate":
                if len(result_df) == 1 and result_df.shape[1] == 1:
                     val = result_df.iloc[0, 0]
                     msg = f"ê³„ì‚° ê²°ê³¼: {val}"
                else:
                     msg = "ìš”ì²­í•˜ì‹  ì§‘ê³„ ê²°ê³¼ì…ë‹ˆë‹¤."
            else:
                msg = "ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."
            
            return result_df, msg, intent, analysis_meta 

        # 2. Level 3: Insight (LLM Required)
        elif intent["type"] == "insight":
            logging.info("[FileEngine] Intent is Insight. Calling LLM.")
            # Insight logic requires schema and data summary
            result_df = df # Default to full df for context
            insight = self._generate_insight(df, result_df, question, state)
            return result_df, insight, intent, analysis_meta
            
        # Default Fallback
        logging.info("[FileEngine] Intent Unclear or Default. Calling LLM for safety.")
        insight = self._generate_insight(df, df, question, state)
        return df, insight, intent, analysis_meta

    def _detect_intent(self, question, state):
        q_lower = question.lower()
        intent = {"type": "insight", "keywords": [], "raw_question": question}
        
        # [Phase 5] 3-Level Intent Detection
        
        # Level 1: Exploration (íƒìƒ‰)
        if any(kw in q_lower for kw in ['ë­˜ ë¬¼ì–´', 'ì–´ë–»ê²Œ ì§ˆë¬¸', 'ë­ë¶€í„°', 'ì´ˆë³´', 'ì–´ë µ', 'ì˜ ëª¨ë¥´']):
            intent["type"] = "guidance"
        elif any(kw in q_lower for kw in ['ì¶”ì´', 'íŠ¸ë Œë“œ', 'ì¼ë³„', 'ì›”ë³„', 'ë³€í™”']):
            intent["type"] = "trend"
        elif any(kw in q_lower for kw in ['ë¹„êµ', 'ëŒ€ë¹„', 'vs', 'ì°¨ì´']):
            intent["type"] = "compare"
        elif any(kw in q_lower for kw in ['êµ¬ì¡°', 'ì»¬ëŸ¼', 'ì—´', 'schema', 'structure']):
             intent["type"] = "schema"
        elif any(kw in q_lower for kw in ['ì–´ë–¤ ë°ì´í„°', 'ë¬´ìŠ¨ ë°ì´í„°', 'ë˜ ì–´ë–¤', 'ì»¬ëŸ¼ ë­', 'í•­ëª© ë­', 'ë­ê°€ ë“¤ì–´', 'ë¬´ì—‡ì´ ë“¤ì–´', 'ì–´ë–¤ê²Œ ìˆì–´']):
             intent["type"] = "columns_summary"
        elif any(kw in q_lower for kw in ['í–‰', 'ìƒ˜í”Œ', 'ì˜ˆì‹œ', 'preview', 'sample', 'ë³´ì—¬ì¤˜', 'raw data']):
            intent["type"] = "preview"
        elif any(kw in q_lower for kw in ['ê°œìš”', 'ìš”ì•½', 'overview', 'summary', 'ì „ì²´']):
            intent["type"] = "overview"
             
        # Level 2: Aggregation (ì§‘ê³„)
        elif any(kw in q_lower for kw in ['ë³„', 'íƒ€ì…ë³„', 'ì¢…ë¥˜ë³„', 'ì¹´í…Œê³ ë¦¬ë³„', 'by ', 'ê·¸ë£¹']):
            intent["type"] = "groupby"
            intent["keywords"] = ["ë³„"]
        elif any(kw in q_lower for kw in ['í‰ê· ', 'average', 'avg', 'mean']):
            intent["type"] = "aggregate"
            intent["keywords"] = ["í‰ê· "]
        elif any(kw in q_lower for kw in ['í•©ê³„', 'ì´', 'sum', 'total']):
            intent["type"] = "aggregate"
            intent["keywords"] = ["í•©ê³„"]
        elif any(kw in q_lower for kw in ['ê°œìˆ˜', 'count', 'ëª‡ ê°œ', 'ëª‡ê°œ']):
            intent["type"] = "aggregate"
            intent["keywords"] = ["ê°œìˆ˜"]

        # íŒŒì¼ ë‚´ ì‚¬ìš©ì/ê´€ë¦¬ì ì§‘ê³„
        elif any(kw in q_lower for kw in ['ì‚¬ìš©ì', 'ìœ ì €', 'íšŒì›', 'ì¸ì›', 'ì‚¬ëŒ']) and any(kw in q_lower for kw in ['ì–¼ë§ˆë‚˜', 'ëª‡', 'ìˆ˜', 'ëª…', 'ëª‡ëª…', 'ëª‡ ëª…']):
            intent["type"] = "count_users"
        elif any(kw in q_lower for kw in ['ì–´ë“œë¯¼', 'ê´€ë¦¬ì', 'admin']) and any(kw in q_lower for kw in ['ì–¼ë§ˆë‚˜', 'ëª‡', 'ìˆ˜']):
            intent["type"] = "count_admin"
        elif any(kw in q_lower for kw in ['ë¬´ìŠ¨ ëœ»', 'ëœ»ì´', 'ì˜ë¯¸', 'ê·¸ê²Œ ë¬´ìŠ¨']):
            intent["type"] = "explain"
            
        # Follow-up detection (Level 3 or Level 2 context)
        elif any(kw in q_lower for kw in ['ì‘', 'ê·¸ë˜', 'ë³´ì—¬ì¤˜', 'ì„¤ëª…í•´ì¤˜']):
            if state.get("last_intent"):
                intent = state["last_intent"]
                intent["is_followup"] = True
            else:
                intent["type"] = "insight"
                
        return intent

    def _execute_aggregation(self, df, intent, state=None):
        state = state or {}
        meta = {}
        # Simplified aggregation for restoration
        if intent["type"] == "schema":
            profile = self._profile_columns(df)
            numeric_cols = [c for c, k in profile.items() if k == "numeric"]
            categorical_cols = [c for c, k in profile.items() if k == "categorical"]
            date_cols = [c for c, k in profile.items() if k == "date"]
            bool_cols = [c for c, k in profile.items() if k == "boolean"]
            id_cols = [c for c, k in profile.items() if k == "identifier"]
            meta["row_count"] = int(len(df))
            meta["col_count"] = int(len(df.columns))
            meta["numeric_count"] = int(len(numeric_cols))
            meta["categorical_count"] = int(len(categorical_cols))
            meta["date_count"] = int(len(date_cols))
            meta["boolean_count"] = int(len(bool_cols))
            meta["identifier_count"] = int(len(id_cols))
            meta["sample_columns"] = [str(c) for c in df.columns[:8]]
            meta["numeric_columns"] = [str(c) for c in numeric_cols]
            meta["categorical_columns"] = [str(c) for c in categorical_cols]
            meta["date_columns"] = [str(c) for c in date_cols]
            meta["boolean_columns"] = [str(c) for c in bool_cols]
            meta["identifier_columns"] = [str(c) for c in id_cols]
            meta["preview_rows"] = self._preview_rows(df, n=5)
            schema_df = pd.DataFrame({
                "column": df.columns,
                "dtype": [str(df[c].dtype) for c in df.columns],
                "null_count": [int(df[c].isna().sum()) for c in df.columns],
                "sample_value": [str(df[c].dropna().iloc[0]) if not df[c].dropna().empty else "" for c in df.columns],
            })
            return schema_df, meta
        elif intent["type"] == "preview":
            return df.head(10), meta
        elif intent["type"] == "overview":
            profile = self._profile_columns(df)
            numeric_cols = [c for c, k in profile.items() if k == "numeric"]
            categorical_cols = [c for c, k in profile.items() if k == "categorical"]
            date_cols = [c for c, k in profile.items() if k == "date"]
            bool_cols = [c for c, k in profile.items() if k == "boolean"]
            id_cols = [c for c, k in profile.items() if k == "identifier"]
            overview = pd.DataFrame({
                "metric": ["row_count", "column_count", "numeric_columns", "categorical_columns", "date_columns", "boolean_columns", "identifier_columns", "missing_cells"],
                "value": [
                    int(len(df)),
                    int(len(df.columns)),
                    int(len(numeric_cols)),
                    int(len(categorical_cols)),
                    int(len(date_cols)),
                    int(len(bool_cols)),
                    int(len(id_cols)),
                    int(df.isna().sum().sum()),
                ]
            })
            meta["preview_rows"] = self._preview_rows(df, n=5)
            return overview, meta
        elif intent["type"] == "columns_summary":
            profile = self._profile_columns(df)
            numeric_cols = [c for c, k in profile.items() if k == "numeric"]
            categorical_cols = [c for c, k in profile.items() if k == "categorical"]
            date_cols = [c for c, k in profile.items() if k == "date"]
            bool_cols = [c for c, k in profile.items() if k == "boolean"]
            id_cols = [c for c, k in profile.items() if k == "identifier"]
            meta["numeric_count"] = len(numeric_cols)
            meta["categorical_count"] = len(categorical_cols)
            meta["date_count"] = len(date_cols)
            meta["boolean_count"] = len(bool_cols)
            meta["identifier_count"] = len(id_cols)
            meta["numeric_columns"] = [str(c) for c in numeric_cols]
            meta["categorical_columns"] = [str(c) for c in categorical_cols]
            meta["date_columns"] = [str(c) for c in date_cols]
            meta["boolean_columns"] = [str(c) for c in bool_cols]
            meta["identifier_columns"] = [str(c) for c in id_cols]
            meta["preview_rows"] = self._preview_rows(df, n=5)
            summary_df = pd.DataFrame({
                "column": df.columns,
                "dtype": [str(df[c].dtype) for c in df.columns],
                "null_count": [int(df[c].isna().sum()) for c in df.columns],
            })
            return summary_df, meta
        elif intent["type"] == "count_users":
            id_col = self._find_user_id_column(df)
            if id_col:
                user_count = int(df[id_col].nunique(dropna=True))
            else:
                user_count = int(len(df))
            meta["user_count"] = user_count
            meta["id_column"] = id_col
            return pd.DataFrame({"user_count": [user_count]}), meta
        elif intent["type"] == "count_admin":
            admin_cols = self._find_admin_columns(df)
            admin_count = 0
            if admin_cols:
                admin_count = int(max(self._count_truthy(df[c]) for c in admin_cols))
            total_count = int(len(df))
            meta["admin_count"] = admin_count
            meta["total_count"] = total_count
            meta["admin_columns"] = admin_cols
            return pd.DataFrame({"admin_count": [admin_count], "total_count": [total_count]}), meta
        elif intent["type"] == "column_count":
            target_col = intent.get("target_column")
            if target_col and target_col in df.columns:
                c = int(df[target_col].dropna().astype(str).replace("", pd.NA).dropna().nunique())
                meta["target_column"] = target_col
                meta["unique_count"] = c
                return pd.DataFrame({"column": [target_col], "unique_count": [c]}), meta
            return pd.DataFrame({"column": [], "unique_count": []}), meta
        elif intent["type"] == "explain":
            last_meta = state.get("last_analysis_meta", {}) if isinstance(state, dict) else {}
            last_intent = (state.get("last_intent") or {}).get("type") if isinstance(state, dict) else None
            explain_text = self._build_explain_text(df, last_intent, last_meta)
            meta["explain_text"] = explain_text
            return pd.DataFrame({"explanation": [explain_text]}), meta
        elif intent["type"] == "guidance":
            guide_text = (
                "ì²˜ìŒì´ë¼ë©´ ì´ë ‡ê²Œ ë¬¼ì–´ë³´ë©´ ë©ë‹ˆë‹¤:\n"
                "1. íŒŒì¼ êµ¬ì¡° ì•Œë ¤ì¤˜\n"
                "2. í•µì‹¬ ì§€í‘œ 3ê°œ ìš”ì•½í•´ì¤˜\n"
                "3. ì±„ë„ë³„/ìœ í˜•ë³„ ë§¤ì¶œ ë¹„êµí•´ì¤˜\n"
                "4. ì¼ë³„ ì¶”ì´ì™€ ì „ì£¼ ëŒ€ë¹„ ë³´ì—¬ì¤˜\n"
                "5. ì´ìƒì¹˜ë‚˜ ê¸‰ë³€ êµ¬ê°„ ì°¾ì•„ì¤˜"
            )
            meta["guide_text"] = guide_text
            return pd.DataFrame({
                "recommended_question": [
                    "íŒŒì¼ êµ¬ì¡° ì•Œë ¤ì¤˜",
                    "í•µì‹¬ ì§€í‘œ 3ê°œ ìš”ì•½í•´ì¤˜",
                    "ì±„ë„ë³„ ë§¤ì¶œ ë¹„êµí•´ì¤˜",
                    "ì¼ë³„ ë§¤ì¶œ ì¶”ì´ ë³´ì—¬ì¤˜",
                    "ì´ìƒì¹˜ ì°¾ì•„ì¤˜"
                ]
            }), meta
        elif intent["type"] == "column_probe":
            target_col = intent.get("target_column")
            n = int(intent.get("preview_count", 5))
            show_unique = bool(intent.get("show_unique", False))
            if show_unique:
                vals = df[target_col].dropna().astype(str).drop_duplicates()
                total_unique = int(len(vals))
                offset = int(intent.get("offset", 0))
                limit = max(1, min(n, 500))
                vals = vals.iloc[offset: offset + limit]
                out = pd.DataFrame({"value": vals.tolist()})
                out.insert(0, "rank", list(range(1, len(out) + 1)))
                meta["target_column"] = target_col
                meta["show_unique"] = True
                meta["total_unique"] = total_unique
                meta["shown_unique"] = int(len(out))
                meta["page_offset"] = offset
                meta["page_limit"] = limit
                meta["has_next"] = (offset + len(out)) < total_unique
                meta["has_prev"] = offset > 0
                return out, meta
            out = df[[target_col]].head(max(1, min(n, 50))).copy()
            out.insert(0, "row_no", list(range(1, len(out) + 1)))
            meta["target_column"] = target_col
            meta["show_unique"] = False
            return out, meta
        elif intent["type"] == "preview_more":
            target_col = intent.get("target_column")
            n = int(intent.get("preview_count", 10))
            if target_col and target_col in df.columns:
                out = df[[target_col]].head(max(1, min(n, 50))).copy()
            else:
                out = df.head(max(1, min(n, 50))).copy()
            out.insert(0, "row_no", list(range(1, len(out) + 1)))
            meta["target_column"] = target_col
            return out, meta
        elif intent["type"] in {"groupby", "distribution", "compare"}:
            group_col = self._guess_group_column(df, intent.get("raw_question", ""))
            metric_col = self._guess_metric_column(df, intent.get("raw_question", ""))
            op = self._guess_op(intent.get("raw_question", ""))
            drop_missing = self._question_wants_drop_missing(intent.get("raw_question", ""))
            if not group_col:
                return df.head(10), meta
            grouped = self._group_aggregate(df, group_col, metric_col, op, drop_missing=drop_missing)
            meta["group_col"] = group_col
            meta["metric_col"] = metric_col or "row_count"
            meta["op"] = op
            meta["drop_missing"] = bool(drop_missing)
            return grouped, meta
        elif intent["type"] == "aggregate":
            metric_col = self._guess_metric_column(df, intent.get("raw_question", ""))
            op = self._guess_op(intent.get("raw_question", ""))
            agg_df = self._aggregate_single(df, metric_col, op)
            meta["metric_col"] = metric_col or "row_count"
            meta["op"] = op
            return agg_df, meta
        elif intent["type"] == "trend":
            date_col = self._guess_date_column(df, intent.get("raw_question", ""))
            metric_col = self._guess_metric_column(df, intent.get("raw_question", ""))
            op = self._guess_op(intent.get("raw_question", ""))
            if not date_col:
                return df.head(10), meta
            trend_df = self._trend_aggregate(df, date_col, metric_col, op)
            meta["date_col"] = date_col
            meta["metric_col"] = metric_col or "row_count"
            meta["op"] = op
            if not trend_df.empty and "date_key" in trend_df.columns:
                meta["period"] = f"{trend_df['date_key'].iloc[0]} ~ {trend_df['date_key'].iloc[-1]}"
            return trend_df, meta

        return df.head(10), meta

    def _transform_to_plot_data(self, df, intent):
        if df is None or df.empty or intent["type"] in ["schema", "preview"]:
            return {"type": None, "labels": [], "series": []}
        if not isinstance(df, pd.DataFrame) or df.shape[1] < 2:
            return {"type": None, "labels": [], "series": []}
        cols = list(df.columns)
        label_col = cols[0]
        numeric_cols = [c for c in cols[1:] if pd.api.types.is_numeric_dtype(df[c])]
        if not numeric_cols:
            return {"type": None, "labels": [], "series": []}
        series_col = numeric_cols[0]
        chart_type = "line" if intent.get("type") == "trend" else "bar"
        return {
            "type": chart_type,
            "labels": [str(v) for v in df[label_col].tolist()],
            "series": [{"name": str(series_col), "data": [float(v) if pd.notna(v) else 0 for v in df[series_col].tolist()]}]
        }

    def _generate_insight(self, df, result_df, question, state=None):
        # ê°€ëŠ¥í•œ ê²½ìš° ë¨¼ì € ê²°ì •ë¡ ì  ìš”ì•½ ì‚¬ìš© (ìˆ«ì í™˜ê° ë°©ì§€)
        deterministic = self._deterministic_summary(df, question)
        if deterministic:
            return deterministic

        schema = df.columns.tolist()
        summary = df.describe().to_json()
        
        prompt = f"""
        Analyze this file data.
        Question: {question}
        Schema: {schema}
        Summary: {summary}
        
        Provide a concise Korean insight based on the data.
        """
        try:
            res = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}]
            )
            return res['choices'][0]['message']['content'].strip()
        except:
            return "íŒŒì¼ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

    def _find_user_id_column(self, df):
        candidates = []
        for c in df.columns:
            cl = str(c).lower()
            if any(k in cl for k in ["user_id", "userid", "member_id", "moc_idx", "uid", "id"]):
                candidates.append(c)
        return candidates[0] if candidates else None

    def _find_admin_columns(self, df):
        cols = []
        for c in df.columns:
            cl = str(c).lower()
            if "admin" in cl or "ê´€ë¦¬ì" in cl:
                cols.append(c)
        return cols

    def _count_truthy(self, series):
        def _truthy(v):
            if pd.isna(v):
                return False
            if isinstance(v, (int, float)):
                return float(v) > 0
            s = str(v).strip().lower()
            return s in {"1", "true", "y", "yes", "t"}
        return int(series.apply(_truthy).sum())

    def _build_explain_text(self, df, last_intent_type, last_meta):
        if last_intent_type == "count_admin":
            admin_count = int(last_meta.get("admin_count", 0))
            total_count = int(last_meta.get("total_count", len(df)))
            ratio = (admin_count / total_count * 100) if total_count else 0.0
            return f"ì¦‰, ì „ì²´ {total_count}ëª… ì¤‘ ê´€ë¦¬ì ê¶Œí•œ ì‚¬ìš©ìëŠ” {admin_count}ëª…({ratio:.1f}%)ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
        if last_intent_type == "count_users":
            user_count = int(last_meta.get("user_count", len(df)))
            return f"ì¦‰, íŒŒì¼ì—ì„œ ì§‘ê³„ ê°€ëŠ¥í•œ ì‚¬ìš©ì ìˆ˜ê°€ {user_count}ëª…ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤."
        if last_intent_type in {"schema", "columns_summary", "overview"}:
            numeric_count = int(last_meta.get("numeric_count", 0))
            categorical_count = int(last_meta.get("categorical_count", 0))
            return (
                f"ì¦‰, ìˆ˜ì¹˜í˜•({numeric_count}ê°œ)ì€ í•©ê³„/í‰ê· /ì¶”ì´ì— ì“°ê³ , "
                f"ë²”ì£¼í˜•({categorical_count}ê°œ)ì€ ì±„ë„ë³„/ìœ í˜•ë³„ì²˜ëŸ¼ ê·¸ë£¹ ë¹„êµì— ì“°ë©´ ë©ë‹ˆë‹¤."
            )
        return "ì¦‰, ì§ì „ ì‘ë‹µì€ íŒŒì¼ì˜ í˜„ì¬ ë°ì´í„° ë¶„í¬ì™€ ì§‘ê³„ ê²°ê³¼ë¥¼ ìš”ì•½í•œ ê²ƒì…ë‹ˆë‹¤."

    def _tokenize(self, text):
        return [t.lower() for t in re.findall(r"[a-zA-Z0-9ê°€-í£_]+", str(text or "")) if len(t) >= 2]

    def _guess_group_column(self, df, question):
        q = str(question or "").lower()
        profile = self._profile_columns(df)
        candidates = []
        for c in df.columns:
            cl = str(c).lower()
            kind = profile.get(c, "categorical")
            if kind == "numeric":
                continue
            score = 0
            if cl in q:
                score += 3
            if "ìœ í˜•" in q and any(k in cl for k in ["type", "ìœ í˜•", "category", "ì¹´í…Œê³ ë¦¬"]):
                score += 3
            if "ì±„ë„" in q and ("channel" in cl or "ì±„ë„" in cl):
                score += 3
            if "êµ­ê°€" in q and ("country" in cl or "êµ­ê°€" in cl):
                score += 3
            if "í›„ì›" in q and ("donation" in cl or "í›„ì›" in cl):
                score += 2
            nunique = int(df[c].nunique(dropna=True))
            if 2 <= nunique <= min(200, len(df)):
                score += 1
            if score > 0:
                candidates.append((score, c))
        if not candidates:
            non_numeric = [c for c in df.columns if not pd.api.types.is_numeric_dtype(df[c])]
            return non_numeric[0] if non_numeric else None
        candidates.sort(key=lambda x: (-x[0], str(x[1])))
        return candidates[0][1]

    def _guess_metric_column(self, df, question):
        q = str(question or "").lower()
        profile = self._profile_columns(df)
        numeric = [c for c in df.columns if profile.get(c) == "numeric"]
        if not numeric:
            return None
        scored = []
        for c in numeric:
            cl = str(c).lower()
            score = 0
            if cl in q:
                score += 4
            if any(k in q for k in ["ë§¤ì¶œ", "ìˆ˜ìµ", "ê¸ˆì•¡", "revenue", "sales"]) and any(k in cl for k in ["revenue", "amount", "sales", "ë§¤ì¶œ", "ìˆ˜ìµ", "ê¸ˆì•¡", "price"]):
                score += 4
            if any(k in q for k in ["ì‚¬ìš©ì", "ìœ ì €", "í›„ì›ì"]) and any(k in cl for k in ["user", "ì‚¬ìš©ì", "ìœ ì €", "member", "buyer", "purchaser"]):
                score += 3
            if any(k in q for k in ["í´ë¦­", "ì´ë²¤íŠ¸", "íšŸìˆ˜", "count"]) and any(k in cl for k in ["count", "event", "click", "íšŸìˆ˜", "ìˆ˜"]):
                score += 3
            scored.append((score, c))
        scored.sort(key=lambda x: (-x[0], str(x[1])))
        return scored[0][1] if scored else numeric[0]

    def _guess_date_column(self, df, question):
        profile = self._profile_columns(df)
        for c in df.columns:
            if profile.get(c) == "date":
                return c
        q = str(question or "").lower()
        for c in df.columns:
            cl = str(c).lower()
            if any(k in cl for k in ["date", "day", "ì¼ì", "ë‚ ì§œ", "yearmonth", "month"]):
                return c
        # fallback: parseable object column
        for c in df.columns:
            if pd.api.types.is_object_dtype(df[c]):
                sample = df[c].dropna().astype(str).head(20)
                if sample.empty:
                    continue
                parsed = pd.to_datetime(sample, errors="coerce")
                if parsed.notna().sum() >= max(3, int(len(sample) * 0.5)):
                    return c
        return None

    def _guess_op(self, question):
        q = str(question or "").lower()
        if any(k in q for k in ["í‰ê· ", "avg", "average", "mean"]):
            return "mean"
        if any(k in q for k in ["ìµœëŒ€", "max", "ê°€ì¥ í°", "highest"]):
            return "max"
        if any(k in q for k in ["ìµœì†Œ", "min", "ê°€ì¥ ì‘ì€", "lowest"]):
            return "min"
        if any(k in q for k in ["ê°œìˆ˜", "count", "ëª‡", "ì–¼ë§ˆë‚˜"]):
            return "count"
        return "sum"

    def _to_numeric_series(self, s):
        if pd.api.types.is_numeric_dtype(s):
            return s
        return pd.to_numeric(s.astype(str).str.replace(r"[^\d\.\-]", "", regex=True), errors="coerce")

    def _group_aggregate(self, df, group_col, metric_col, op, drop_missing=False):
        work = df.copy()
        if drop_missing and group_col in work.columns:
            s = work[group_col].astype(str).str.strip()
            mask = work[group_col].notna() & ~s.str.lower().isin({"", "(not set)", "not set", "none", "null", "nan"})
            work = work[mask]
        if metric_col:
            work[metric_col] = self._to_numeric_series(work[metric_col])
        g = work.groupby(group_col, dropna=False)
        if op == "count" or not metric_col:
            out = g.size().reset_index(name="count")
        elif op == "mean":
            out = g[metric_col].mean().reset_index(name=f"{metric_col}_mean")
        elif op == "max":
            out = g[metric_col].max().reset_index(name=f"{metric_col}_max")
        elif op == "min":
            out = g[metric_col].min().reset_index(name=f"{metric_col}_min")
        else:
            out = g[metric_col].sum().reset_index(name=f"{metric_col}_sum")
        val_col = out.columns[-1]
        out = out.sort_values(val_col, ascending=False).reset_index(drop=True)
        return out.head(200)

    def _profile_columns(self, df):
        profile = {}
        for c in df.columns:
            profile[c] = self._infer_col_kind(df, c)
        return profile

    def _infer_col_kind(self, df, col):
        s = df[col]
        cl = str(col).lower()
        non_null = s.dropna()
        if non_null.empty:
            return "categorical"

        if pd.api.types.is_bool_dtype(s):
            return "boolean"

        sample = non_null.astype(str).str.strip().head(2000)
        lowered = sample.str.lower()
        bool_vals = {"y", "n", "yes", "no", "true", "false", "0", "1", "t", "f"}
        if len(sample) >= 5 and lowered.isin(bool_vals).mean() >= 0.95:
            return "boolean"

        with warnings.catch_warnings():
            warnings.simplefilter("ignore", UserWarning)
            parsed_dt = pd.to_datetime(sample, errors="coerce")
        if parsed_dt.notna().mean() >= 0.9:
            return "date"

        numeric_s = pd.to_numeric(sample.str.replace(r"[^\d\.\-]", "", regex=True), errors="coerce")
        numeric_ratio = numeric_s.notna().mean()
        if numeric_ratio >= 0.95:
            # identifier heuristic: column name + uniqueness + integer-like
            uniq_ratio = float(sample.nunique(dropna=True)) / max(1, len(sample))
            uniq_count = int(sample.nunique(dropna=True))
            integer_like = ((numeric_s.dropna() % 1) == 0).mean() >= 0.98 if numeric_s.dropna().shape[0] else False
            id_name = any(k in cl for k in ["id", "_id", "idx", "ì½”ë“œ", "ë²ˆí˜¸", "no", "seq", "key"])
            code_name = any(k in cl for k in ["route", "type", "category", "status", "grade", "level", "group", "êµ¬ë¶„", "ìœ í˜•", "ë“±ê¸‰", "ìƒíƒœ", "ê²½ë¡œ"])
            low_card_code = integer_like and uniq_count <= 20 and uniq_ratio <= 0.4
            seq_like = False
            if integer_like and numeric_s.dropna().shape[0] >= 3:
                vals = np.sort(numeric_s.dropna().astype(float).values)
                diffs = np.diff(vals)
                seq_like = bool(len(diffs) > 0 and (diffs == 1).mean() >= 0.95)
            # ì½”ë“œ/ë¼ë²¨ ì„±ê²© ìˆ«ìëŠ” ê³„ì‚° ëŒ€ìƒì´ ì•„ë‹ˆë¼ ë²”ì£¼ë¡œ ì·¨ê¸‰
            if code_name or low_card_code:
                return "categorical"
            if id_name or seq_like:
                return "identifier"
            return "numeric"
        return "categorical"

    def _question_wants_drop_missing(self, question):
        q = str(question or "").lower()
        return any(k in q for k in ["ê²°ì¸¡ ì œì™¸", "ê²°ì¸¡ì¹˜ ì œì™¸", "null ì œì™¸", "not set ì œì™¸", "(not set) ì œì™¸", "ë¹ˆê°’ ì œì™¸", "ëˆ„ë½ ì œì™¸"])

    def _aggregate_single(self, df, metric_col, op):
        if not metric_col:
            return pd.DataFrame({"row_count": [int(len(df))]})
        s = self._to_numeric_series(df[metric_col])
        if op == "count":
            return pd.DataFrame({f"{metric_col}_count": [int(s.notna().sum())]})
        if op == "mean":
            return pd.DataFrame({f"{metric_col}_mean": [float(s.mean(skipna=True) if s.notna().any() else 0)]})
        if op == "max":
            return pd.DataFrame({f"{metric_col}_max": [float(s.max(skipna=True) if s.notna().any() else 0)]})
        if op == "min":
            return pd.DataFrame({f"{metric_col}_min": [float(s.min(skipna=True) if s.notna().any() else 0)]})
        return pd.DataFrame({f"{metric_col}_sum": [float(s.sum(skipna=True) if s.notna().any() else 0)]})

    def _trend_aggregate(self, df, date_col, metric_col, op):
        work = df.copy()
        work[date_col] = pd.to_datetime(work[date_col], errors="coerce")
        work = work[work[date_col].notna()].copy()
        if work.empty:
            return pd.DataFrame(columns=[date_col, "value"])
        work["date_key"] = work[date_col].dt.date.astype(str)
        if metric_col:
            work[metric_col] = self._to_numeric_series(work[metric_col])
        g = work.groupby("date_key", dropna=False)
        if op == "count" or not metric_col:
            out = g.size().reset_index(name="count")
        elif op == "mean":
            out = g[metric_col].mean().reset_index(name=f"{metric_col}_mean")
        elif op == "max":
            out = g[metric_col].max().reset_index(name=f"{metric_col}_max")
        elif op == "min":
            out = g[metric_col].min().reset_index(name=f"{metric_col}_min")
        else:
            out = g[metric_col].sum().reset_index(name=f"{metric_col}_sum")
        out = out.sort_values("date_key", ascending=True).reset_index(drop=True)
        return out.head(400)

    def _build_followups(self, df, intent, meta):
        t = intent.get("type")
        if t == "schema":
            return [
                "í•µì‹¬ ì§€í‘œ 3ê°œë¥¼ ë¨¼ì € ìš”ì•½í•´ë³¼ê¹Œìš”?",
                "ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜/ì´ìƒì¹˜ë¥¼ ì ê²€í•´ë³¼ê¹Œìš”?",
                "ìƒ˜í”Œ 10í–‰ ë” ë³´ê¸°"
            ]
        if t == "column_probe":
            f = []
            if meta.get("show_unique"):
                f.append("ê³ ìœ ê°’ ì „ì²´ ëª©ë¡ ë³´ê¸°")
                if meta.get("has_next"):
                    f.append("ë‹¤ìŒ 500ê°œ ë³´ê¸°")
                if meta.get("has_prev"):
                    f.append("ì´ì „ 500ê°œ ë³´ê¸°")
                f += ["ìƒ˜í”Œ 10í–‰ ë” ë³´ê¸°", "ë‹¤ë¥¸ ì»¬ëŸ¼ê³¼ êµì°¨ ì§‘ê³„í•´ë³¼ê¹Œìš”?"]
                return f[:5]
            return [
                "ìƒ˜í”Œ 10í–‰ ë” ë³´ê¸°",
                "ê³ ìœ ê°’ ì „ì²´ ëª©ë¡ ë³´ê¸°",
                "í•´ë‹¹ ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ê°œìˆ˜ë„ ë³¼ê¹Œìš”?",
                "ë¹ˆê°’/ì´ìƒê°’ ë¹„ìœ¨ë„ ì ê²€í•´ë³¼ê¹Œìš”?",
                "ë‹¤ë¥¸ ì»¬ëŸ¼ê³¼ êµì°¨ ì§‘ê³„í•´ë³¼ê¹Œìš”?"
            ]
        if t == "column_count":
            return [
                "ê³ ìœ ê°’ ëª©ë¡ë„ ë³´ì—¬ì¤„ê¹Œìš”?",
                "ê²°ì¸¡ì¹˜ë¥¼ ì œì™¸í•˜ê³  ë‹¤ì‹œ ë³¼ê¹Œìš”?",
                "ë‹¤ë¥¸ ì»¬ëŸ¼ê³¼ êµì°¨ ì§‘ê³„í•´ë³¼ê¹Œìš”?"
            ]
        if t in {"groupby", "distribution", "compare"}:
            return [
                "ìƒìœ„ 10ê°œ í•­ëª©ë§Œ ì¶”ë ¤ì„œ ë³¼ê¹Œìš”?",
                "ì´ì „ ê¸°ê°„ê³¼ ë¹„êµí•  ìˆ˜ ìˆê²Œ ì¶”ì´ë¡œ ë°”ê¿”ë³¼ê¹Œìš”?",
                "ë¹„ì¤‘(%) ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬í•´ë³¼ê¹Œìš”?"
            ]
        if t == "trend":
            return [
                "ì „ì£¼/ì „ì›”ê³¼ ë¹„êµí•´ ì¦ê°ë¥ ì„ ë³¼ê¹Œìš”?",
                "ì¶”ì´ì—ì„œ ê¸‰ì¦/ê¸‰ê° êµ¬ê°„ë§Œ ë½‘ì•„ë³¼ê¹Œìš”?",
                "ì±„ë„/ìœ í˜•ìœ¼ë¡œ ë¶„í•´í•´ì„œ ì¶”ì´ë¥¼ ë³¼ê¹Œìš”?"
            ]
        return [
            "ìš”ì•½ë¶€í„° ë³¼ê¹Œìš”?",
            "ì§‘ê³„(í•©ê³„/í‰ê· /ê°œìˆ˜)ë¡œ ë³¼ê¹Œìš”?",
            "ì¹´í…Œê³ ë¦¬ë³„ ë¹„êµë¡œ ë³¼ê¹Œìš”?"
        ]

    def _is_preview_more_request(self, question):
        q = str(question or "").lower()
        return any(k in q for k in ["ë” ë³´ê¸°", "ë”ë³´ì—¬", "ì¶”ê°€ë¡œ ë³´ì—¬", "ìƒ˜í”Œ 10", "10í–‰"])

    def _detect_page_request(self, question):
        q = str(question or "").lower()
        next_kw = ["ë‹¤ìŒ", "ê³„ì†", "ì´ì–´", "more", "next"]
        prev_kw = ["ì´ì „", "ì•", "prev", "previous"]
        all_kw = ["500", "ê°œ", "ë³´ê¸°", "í˜ì´ì§€", "ì „ì²´", "ëª©ë¡"]

        if any(k in q for k in next_kw) and any(k in q for k in all_kw):
            return {"direction": "next", "size": 500}
        if any(k in q for k in prev_kw) and any(k in q for k in all_kw):
            return {"direction": "prev", "size": 500}
        return None

    def _preview_rows(self, df, n=5):
        n = max(1, min(int(n), 10))
        part = df.head(n).copy()
        # ê°€ë…ì„±ì„ ìœ„í•´ ë¬¸ìì—´ ê¸¸ì´ ì œí•œ
        for c in part.columns:
            part[c] = part[c].apply(lambda v: (str(v)[:40] + "â€¦") if len(str(v)) > 40 else v)
        return part.where(pd.notnull(part), None).to_dict(orient="records")

    def _build_preview_tail(self, analysis_meta):
        rows = analysis_meta.get("preview_rows") or []
        if not rows:
            return ""
        lines = ["\nìƒ˜í”Œ 1~5í–‰ ë¯¸ë¦¬ë³´ê¸°:"]
        for i, row in enumerate(rows[:5], 1):
            pairs = [f"{k}={row.get(k)}" for k in list(row.keys())[:4]]
            lines.append(f"{i}) " + ", ".join(pairs))
        return "\n" + "\n".join(lines)

    def _detect_column_probe(self, df, question):
        q = str(question or "").lower()
        ask_value = any(k in q for k in ["ì–´ë–¤ ë°ì´í„°", "ì–´ë–¤ ê°’", "ê°’ì´ ë­", "ë‚´ìš©ì´ ë­", "ìƒ˜í”Œ", "ë¯¸ë¦¬ë³´ê¸°", "1-5", "1~5", "5í–‰", "ëª©ë¡", "ì¢…ë¥˜", "ì „ì²´", "ëª¨ë‘", "ê³ ìœ ê°’"])
        if not ask_value:
            return None
        best = None
        for c in df.columns:
            cl = str(c).lower()
            if cl and (cl in q or str(c) in question):
                best = c
                break
        if not best:
            return None
        m = re.search(r"(\d+)\s*[-~]\s*(\d+)", q)
        preview_count = 5
        if m:
            try:
                s, e = int(m.group(1)), int(m.group(2))
                if e >= s:
                    preview_count = max(1, min(e - s + 1, 20))
            except Exception:
                pass
        if any(k in q for k in ["ì „ì²´", "ëª¨ë‘", "ëª©ë¡", "ì¢…ë¥˜", "ê³ ìœ ê°’"]):
            preview_count = 500
        show_unique = any(k in q for k in ["ì „ì²´", "ëª¨ë‘", "ëª©ë¡", "ì¢…ë¥˜", "ê³ ìœ ê°’"])
        return {"target_column": best, "preview_count": preview_count, "show_unique": show_unique}

    def _detect_column_count(self, df, question):
        q = str(question or "").lower()
        if not any(k in q for k in ["ëª‡ê°œ", "ëª‡ ê°œ", "ê°œìˆ˜", "ê³ ìœ ê°’", "unique"]):
            return None
        best = None
        for c in df.columns:
            cl = str(c).lower()
            if cl and (cl in q or str(c) in question):
                best = c
                break
        if not best:
            alias_map = {"íšŒì›ë²ˆí˜¸": ["íšŒì›ë²ˆí˜¸", "member_no", "memberid", "member_id", "moc_idx", "user_id", "uid", "id"]}
            for _, aliases in alias_map.items():
                if any(a in q for a in aliases):
                    for c in df.columns:
                        cl = str(c).lower()
                        if any(a in cl for a in aliases):
                            best = c
                            break
                if best:
                    break
        return best

    def _make_beginner_message(self, message, intent, meta):
        t = (intent or {}).get("type", "")
        msg = str(message or "")
        if t == "groupby":
            g = meta.get("group_col")
            m = meta.get("metric_col")
            return msg + f"\n\nì‰½ê²Œ ë§í•˜ë©´: `{g}`ë³„ë¡œ `{m}`ë¥¼ ë¬¶ì–´ì„œ ë¹„êµí•œ ê²°ê³¼ì…ë‹ˆë‹¤."
        if t == "compare":
            g = meta.get("group_col")
            m = meta.get("metric_col")
            return msg + f"\n\nì‰½ê²Œ ë§í•˜ë©´: `{g}` ê·¸ë£¹ë¼ë¦¬ `{m}` ê°’ ì°¨ì´ë¥¼ ë¹„êµí•œ ê²°ê³¼ì…ë‹ˆë‹¤."
        if t == "aggregate":
            m = meta.get("metric_col")
            op = meta.get("op")
            return msg + f"\n\nì‰½ê²Œ ë§í•˜ë©´: `{m}`ì— `{op}` ê³„ì‚°ì„ ì ìš©í•œ ë‹¨ì¼ ìš”ì•½ê°’ì…ë‹ˆë‹¤."
        if t == "trend":
            p = meta.get("period")
            return msg + f"\n\nì‰½ê²Œ ë§í•˜ë©´: ì‹œê°„ íë¦„ì— ë”°ë¼ ê°’ì´ ì–´ë–»ê²Œ ë°”ë€ŒëŠ”ì§€ ë³¸ ê²ƒì…ë‹ˆë‹¤. ({p})"
        if t in {"schema", "columns_summary", "overview"}:
            return msg + "\n\níŒ: ìˆ˜ì¹˜í˜•ì€ ê³„ì‚°(í•©ê³„/í‰ê· ), ë²”ì£¼í˜•ì€ ë¹„êµ(~ë³„)ì— ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤."
        return msg

    def _raw_limit_by_intent(self, intent, meta):
        t = (intent or {}).get("type")
        if t in {"schema", "columns_summary"}:
            return 200
        if t in {"preview_more", "column_probe"}:
            return 500
        if t in {"column_count"}:
            return 10
        if t in {"groupby", "distribution", "compare", "trend"}:
            return 300
        return 100

    def _infer_dataset_period(self, df):
        date_col = self._guess_date_column(df, "")
        if not date_col or date_col not in df.columns:
            return None
        ser = pd.to_datetime(df[date_col], errors="coerce")
        ser = ser[ser.notna()]
        if ser.empty:
            return None
        start = ser.min().date().isoformat()
        end = ser.max().date().isoformat()
        return f"{start} ~ {end}"

    def _deterministic_summary(self, df, question):
        q = (question or "").lower()
        if any(k in q for k in ["ì‚¬ìš©ì", "ìœ ì €", "íšŒì›", "ì¸ì›", "ì‚¬ëŒ"]) and any(k in q for k in ["ì–¼ë§ˆë‚˜", "ëª‡", "ìˆ˜", "ëª…", "ëª‡ëª…", "ëª‡ ëª…"]):
            id_col = self._find_user_id_column(df)
            user_count = int(df[id_col].nunique(dropna=True)) if id_col else int(len(df))
            return f"ì´ íŒŒì¼ ê¸°ì¤€ ì‚¬ìš©ì ìˆ˜ëŠ” **{user_count}ëª…**ì…ë‹ˆë‹¤."
        if any(k in q for k in ["íšŒì›ë²ˆí˜¸", "íšŒì› ë²ˆí˜¸", "id", "ë²ˆí˜¸"]) and any(k in q for k in ["ëª‡ê°œ", "ëª‡ ê°œ", "ê°œìˆ˜", "ê³ ìœ "]):
            for c in df.columns:
                cl = str(c).lower()
                if any(t in cl for t in ["member", "íšŒì›", "moc_idx", "user_id", "uid", "id", "ë²ˆí˜¸"]):
                    cnt = int(df[c].dropna().astype(str).replace("", pd.NA).dropna().nunique())
                    return f"`{c}` ê¸°ì¤€ ê³ ìœ  ê°œìˆ˜ëŠ” **{cnt:,}ê°œ**ì…ë‹ˆë‹¤."
        if any(k in q for k in ["ì–´ë“œë¯¼", "ê´€ë¦¬ì", "admin"]) and any(k in q for k in ["ì–¼ë§ˆë‚˜", "ëª‡", "ìˆ˜"]):
            admin_cols = self._find_admin_columns(df)
            admin_count = int(max(self._count_truthy(df[c]) for c in admin_cols)) if admin_cols else 0
            total_count = int(len(df))
            ratio = (admin_count / total_count * 100) if total_count else 0.0
            return f"ê´€ë¦¬ì(ì–´ë“œë¯¼) ìˆ˜ëŠ” **{admin_count}ëª…**ì´ë©°, ì „ì²´ ëŒ€ë¹„ **{ratio:.1f}%**ì…ë‹ˆë‹¤."
        if any(k in q for k in ["ì–´ë–¤ ë°ì´í„°", "ë¬´ìŠ¨ ë°ì´í„°", "ë˜ ì–´ë–¤"]):
            numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]
            categorical_cols = [c for c in df.columns if c not in numeric_cols]
            sample_cols = ", ".join([str(c) for c in df.columns[:8]])
            return (
                f"ì´ íŒŒì¼ì€ ì´ **{len(df)}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼**ì…ë‹ˆë‹¤. "
                f"ìˆ˜ì¹˜í˜• {len(numeric_cols)}ê°œ, ë²”ì£¼í˜• {len(categorical_cols)}ê°œì´ë©°, "
                f"ëŒ€í‘œ ì»¬ëŸ¼ì€ `{sample_cols}` ì…ë‹ˆë‹¤."
            )
        return ""

file_engine = FileAnalysisEngine()
